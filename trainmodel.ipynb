{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module to support machine learning of activity states from acc data\"\"\"\n",
    "\n",
    "from accelerometer import utils\n",
    "from accelerometer.models import MODELS\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.ensemble._forest as forest\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "import tarfile\n",
    "import warnings\n",
    "import urllib\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "\n",
    "def activityClassification(epochFile, activityModel=\"walmsley\"):\n",
    "    \"\"\"Perform classification of activity states from epoch feature data\n",
    "\n",
    "    Based on a balanced random forest with a Hidden Markov Model containing\n",
    "    transitions between predicted activity states and emissions trained using a\n",
    "    free-living groundtruth to identify pre-defined classes of behaviour from\n",
    "    accelerometer data.\n",
    "\n",
    "    :param str epochFile: Input csv file of processed epoch data\n",
    "    :param str activityModel: Input tar model file which contains random forest\n",
    "        pickle model, HMM priors/transitions/emissions npy files, and npy file\n",
    "        of METs for each activity state\n",
    "\n",
    "    :return: Pandas dataframe of activity epoch data with one-hot encoded labels\n",
    "    :rtype: pandas.DataFrame\n",
    "\n",
    "    :return: Activity state labels\n",
    "    :rtype: list(str)\n",
    "    \"\"\"\n",
    "\n",
    "    activityModel = resolveModelPath(activityModel)\n",
    "\n",
    "    X = epochFile\n",
    "    featureColsFile = getFileFromTar(activityModel, 'featureCols.txt').getvalue()\n",
    "    featureColsList = featureColsFile.decode().split('\\n')\n",
    "    featureCols = list(filter(None, featureColsList))\n",
    "\n",
    "    with pd.option_context('mode.use_inf_as_null', True):\n",
    "        null_rows = X[featureCols].isnull().any(axis=1)\n",
    "    print(null_rows.sum(), \"rows with missing (NaN, None, or NaT) or Inf values, out of\", len(X))\n",
    "\n",
    "    X['label'] = 'none'\n",
    "    X.loc[null_rows, 'label'] = 'inf_or_null'\n",
    "    # Setup RF\n",
    "    # Ignore warnings on deployed model using different version of pandas\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "        rf = joblib.load(getFileFromTar(activityModel, 'rfModel.pkl'))\n",
    "    labels = rf.classes_.tolist()\n",
    "    rfPredictions = rf.predict(X.loc[~null_rows, featureCols].to_numpy())\n",
    "    # Free memory\n",
    "    del rf\n",
    "    # Setup HMM\n",
    "    priors = np.load(getFileFromTar(activityModel, 'hmmPriors.npy'))\n",
    "    transitions = np.load(getFileFromTar(activityModel, 'hmmTransitions.npy'))\n",
    "    emissions = np.load(getFileFromTar(activityModel, 'hmmEmissions.npy'))\n",
    "    hmmPredictions = viterbi(rfPredictions.tolist(), labels, priors,\n",
    "                             transitions, emissions)\n",
    "    # Save predictions to pandas dataframe\n",
    "    X.loc[~null_rows, 'label'] = hmmPredictions\n",
    "\n",
    "    # Perform MET prediction...\n",
    "    # Pandas .replace method has a small bug\n",
    "    # See https://github.com/pandas-dev/pandas/issues/23305\n",
    "    # We need to force type\n",
    "    met_vals = np.load(getFileFromTar(activityModel, 'METs.npy'))\n",
    "    met_dict = dict(zip(labels, met_vals))\n",
    "    X.loc[~null_rows, 'MET'] = X.loc[~null_rows, 'label'].replace(met_dict).astype('float')\n",
    "\n",
    "    # Apply one-hot encoding\n",
    "    for l in labels:\n",
    "        X[l] = 0\n",
    "        X.loc[X['label'] == l, l] = 1\n",
    "    # Null values aren't one-hot encoded, so set such instances to NaN\n",
    "    for l in labels:\n",
    "        X.loc[X[labels].sum(axis=1) == 0, l] = np.nan\n",
    "    return X, labels\n",
    "\n",
    "\n",
    "MIN_TRAIN_CLASS_COUNT = 100\n",
    "\n",
    "\n",
    "def trainClassificationModel(\n",
    "        trainingFile,\n",
    "        labelCol=\"label\", participantCol=\"participant\",\n",
    "        atomicLabelCol=\"annotation\", metCol=\"MET\",\n",
    "        featuresTxt=\"activityModels/features.txt\",\n",
    "        trainParticipants=None, testParticipants=None,\n",
    "        rfThreads=1, rfTrees=1000, rfFeats=None, rfDepth=None,\n",
    "        outputPredict=\"activityModels/test-predictions.csv\",\n",
    "        outputModel=None\n",
    "):\n",
    "    \"\"\"Train model to classify activity states from epoch feature data\n",
    "\n",
    "    Based on a balanced random forest with a Hidden Markov Model containing\n",
    "    transitions between predicted activity states and emissions trained using\n",
    "    the input training file to identify pre-defined classes of behaviour from\n",
    "    accelerometer data.\n",
    "\n",
    "    :param str trainingFile: Input csv file of training data, pre-sorted by time\n",
    "    :param str labelCol: Input label column\n",
    "    :param str participantCol: Input participant column\n",
    "    :param str atomicLabelCol: Input 'atomic' annotation e.g. 'walking with dog'\n",
    "        vs. 'walking'\n",
    "    :param str metCol: Input MET column\n",
    "    :param str featuresTxt: Input txt file listing feature column names\n",
    "    :param str trainParticipants: Input comma separated list of participant IDs\n",
    "        to train on.\n",
    "    :param str testParticipants: Input comma separated list of participant IDs\n",
    "        to test on.\n",
    "    :param int rfThreads: Input num threads to use when training random forest\n",
    "    :param int rfTrees: Input num decision trees to include in random forest\n",
    "    :param str outputPredict: Output CSV of person, label, predicted\n",
    "    :param str outputModel: Output tarfile object which contains random forest\n",
    "        pickle model, HMM priors/transitions/emissions npy files, and npy file\n",
    "        of METs for each activity state. Will only output trained model if this\n",
    "        is not null e.g. \"activityModels/sample-model.tar\"\n",
    "\n",
    "    :return: New model written to <outputModel> OR csv of test predictions\n",
    "        written to <outputPredict>\n",
    "    :rtype: void\n",
    "    \"\"\"\n",
    "\n",
    "    # Load list of features to use in analysis\n",
    "    featureCols = getListFromTxtFile(featuresTxt)\n",
    "\n",
    "    # Load in participant information, and remove null/messy labels/features\n",
    "    train = pd.read_csv(trainingFile)\n",
    "    \n",
    "    #!!!ADDED W/ JAMES\n",
    "    train[participantCol] = train[participantCol].apply(lambda x: str(x))\n",
    "        \n",
    "    train = train[~pd.isnull(train[labelCol])]\n",
    "    allCols = [participantCol, labelCol, atomicLabelCol, metCol] + featureCols\n",
    "    with pd.option_context('mode.use_inf_as_null', True):\n",
    "        train = train[allCols].dropna(axis=0, how='any')\n",
    "\n",
    "    \n",
    "    # Reduce size of train/test sets if we are training/testing on some people\n",
    "    if testParticipants is not None:\n",
    "        testPIDs = testParticipants.split(',')\n",
    "        test = train[train[participantCol].isin(testPIDs)]\n",
    "        train = train[~train[participantCol].isin(testPIDs)]\n",
    "    if trainParticipants is not None:\n",
    "        trainPIDs = trainParticipants.split(',')\n",
    "        train = train[train[participantCol].isin(trainPIDs)]\n",
    "\n",
    "    # Train Random Forest model\n",
    "    # First \"monkeypatch\" RF function to perform per-class balancing\n",
    "    global MIN_TRAIN_CLASS_COUNT\n",
    "    MIN_TRAIN_CLASS_COUNT = train[labelCol].value_counts().min()\n",
    "    forest._parallel_build_trees = _parallel_build_trees\n",
    "    # Then train RF model (which include per-class balancing)\n",
    "    rfClassifier = RandomForestClassifier(n_estimators=rfTrees,\n",
    "                                          n_jobs=rfThreads,\n",
    "                                          max_features=rfFeats,\n",
    "                                          max_depth=rfDepth,\n",
    "                                          oob_score=True)\n",
    "\n",
    "    rfModel = rfClassifier.fit(train[featureCols], train[labelCol].tolist())\n",
    "\n",
    "    # Train Hidden Markov Model\n",
    "    states, priors, emissions, transitions = train_HMM(rfModel, train[labelCol], labelCol)\n",
    "    rfModel.oob_decision_function_ = None  # out of bound errors are no longer needed\n",
    "\n",
    "    # Estimate usual METs-per-class\n",
    "    METs = []\n",
    "    for s in states:\n",
    "        MET = train[train[labelCol] == s].groupby(atomicLabelCol)[metCol].mean().mean()\n",
    "        METs += [MET]\n",
    "\n",
    "    # Now write out model\n",
    "    if outputModel is not None:\n",
    "        saveModelsToTar(outputModel, featureCols, rfModel, priors, transitions, emissions, METs)\n",
    "\n",
    "    # Assess model performance on test participants\n",
    "    if testParticipants is not None:\n",
    "        print('test on participant(s):, ', testParticipants)\n",
    "        labels = rfModel.classes_.tolist()\n",
    "        rfPredictions = rfModel.predict(test[featureCols])\n",
    "        hmmPredictions = viterbi(rfPredictions.tolist(), labels, priors,\n",
    "                                 transitions, emissions)\n",
    "        test['predicted'] = hmmPredictions\n",
    "        # And write out to file\n",
    "        outCols = [participantCol, labelCol, 'predicted']\n",
    "        test[outCols].to_csv(outputPredict, index=False)\n",
    "        print('Output predictions written to: ', outputPredict)\n",
    "\n",
    "\n",
    "def train_HMM(rfModel, y_trainF, labelCol):\n",
    "    \"\"\"Train Hidden Markov Model\n",
    "\n",
    "    Use data not considered in construction of random forest to estimate\n",
    "    probabilities of: i) starting in a given state; ii) transitioning from\n",
    "    one state to another; and iii) probabilitiy of the random forest being\n",
    "    correct when predicting a given class (emission probability)\n",
    "\n",
    "    :param sklearn.RandomForestClassifier rfModel: Input random forest object\n",
    "    :param dataframe.Column y_trainF: Input groundtruth for each intance\n",
    "    :param str labelCol: Input label column\n",
    "\n",
    "    :return: states - List of unique activity state labels\n",
    "    rtype: numpy.array\n",
    "\n",
    "    :return: priors - Prior probabilities for each activity state\n",
    "    rtype: numpy.array\n",
    "\n",
    "    :return: transitions - Probability matrix of transitioning from one activity\n",
    "        state to another\n",
    "    rtype: numpy.array\n",
    "\n",
    "    :return: emissions - Probability matrix of RF prediction being true\n",
    "    rtype: numpy.array\n",
    "    \"\"\"\n",
    "\n",
    "    states = rfModel.classes_\n",
    "\n",
    "    # Get out of bag (OOB) predictions from Random Forest\n",
    "    predOOB = pd.DataFrame(rfModel.oob_decision_function_)\n",
    "    predOOB.columns = states\n",
    "    predOOB['labelOOB'] = predOOB.idxmax(axis=1)\n",
    "    predOOB['groundTruth'] = y_trainF.values\n",
    "\n",
    "    # Initial state probabilities\n",
    "    prior = []\n",
    "    for s in states:\n",
    "        sProb = len(y_trainF[y_trainF == s]) / (len(y_trainF) * 1.0)\n",
    "        prior += [sProb]\n",
    "\n",
    "    # Emission probabilities\n",
    "    emissions = np.zeros((len(states), len(states)))\n",
    "    j = 0\n",
    "    for predictedState in states:\n",
    "        k = 0\n",
    "        for actualState in states:\n",
    "            emissions[j, k] = predOOB[actualState][predOOB['groundTruth'] == predictedState].sum()\n",
    "            emissions[j, k] /= len(predOOB[predOOB['groundTruth'] == predictedState])\n",
    "            k += 1\n",
    "        j += 1\n",
    "\n",
    "    # Transition probabilities\n",
    "    train = y_trainF.to_frame()\n",
    "    train['nextLabel'] = train[labelCol].shift(-1)\n",
    "    transitions = np.zeros((len(states), len(states)))\n",
    "    j = 0\n",
    "    for s1 in states:\n",
    "        k = 0\n",
    "        for s2 in states:\n",
    "            transitions[j, k] = len(train[(train[labelCol] == s1) & (train['nextLabel'] == s2)]\n",
    "                                    ) / (len(train[train[labelCol] == s1]) * 1.0)\n",
    "            k += 1\n",
    "        j += 1\n",
    "\n",
    "    # Return HMM matrices\n",
    "    return states, prior, emissions, transitions\n",
    "\n",
    "\n",
    "def viterbi(observations, states, priors, transitions, emissions,\n",
    "            probabilistic=False):\n",
    "    \"\"\"Perform HMM smoothing over observations via Viteri algorithm\n",
    "\n",
    "    :param list(str) observations: List/sequence of activity states\n",
    "    :param numpy.array states: List of unique activity state labels\n",
    "    :param numpy.array priors: Prior probabilities for each activity state\n",
    "    :param numpy.array transitions: Probability matrix of transitioning from one\n",
    "        activity state to another\n",
    "    :param numpy.array emissions: Probability matrix of RF prediction being true\n",
    "    :param bool probabilistic: Write probabilistic output for each state, rather\n",
    "        than writing most likely state for any given prediction.\n",
    "\n",
    "    :return: Smoothed list/sequence of activity states\n",
    "    :rtype: list(str)\n",
    "    \"\"\"\n",
    "\n",
    "    def norm(x):\n",
    "        return x / x.sum()\n",
    "\n",
    "    tinyNum = 0.000001\n",
    "    nObservations = len(observations)\n",
    "    nStates = len(states)\n",
    "    v = np.zeros((nObservations, nStates))  # initialise viterbi table\n",
    "    # Set prior state values for first observation...\n",
    "    for state in range(0, len(states)):\n",
    "        v[0, state] = np.log(priors[state] * emissions[state, states.index(observations[0])] + tinyNum)\n",
    "    # Fill in remaning matrix observations\n",
    "    # Use log space as multiplying successively smaller p values)\n",
    "    for k in range(1, nObservations):\n",
    "        for state in range(0, len(states)):\n",
    "            v[k, state] = np.log(emissions[state, states.index(observations[k])] + tinyNum) + \\\n",
    "                np.max(v[k - 1, :] + np.log(transitions[:, state] + tinyNum), axis=0)\n",
    "\n",
    "    # Now construct viterbiPath (propagating backwards)\n",
    "    viterbiPath = observations\n",
    "    # Pick most probable state for final observation\n",
    "    viterbiPath[nObservations - 1] = states[np.argmax(v[nObservations - 1, :], axis=0)]\n",
    "\n",
    "    # Probabilistic method will give probability of each label\n",
    "    if probabilistic:\n",
    "        viterbiProba = np.zeros((nObservations, nStates))  # initialize table\n",
    "        viterbiProba[nObservations - 1, :] = norm(v[nObservations - 1, :])\n",
    "\n",
    "    # And then work backwards to pick most probable state for all other observations\n",
    "    for k in list(reversed(range(0, nObservations - 1))):\n",
    "        viterbiPath[k] = states[np.argmax(\n",
    "            v[k, :] + np.log(transitions[:, states.index(viterbiPath[k + 1])] + tinyNum), axis=0)]\n",
    "        if probabilistic:\n",
    "            viterbiProba[k, :] = norm(v[k, :] + np.log(transitions[:, states.index(viterbiPath[k + 1])] + tinyNum))\n",
    "\n",
    "    # Output as list...\n",
    "    return viterbiProba if probabilistic else viterbiPath\n",
    "\n",
    "\n",
    "GLOBAL_INDICES = []\n",
    "\n",
    "\n",
    "def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees,\n",
    "                          verbose=0, class_weight=None, n_samples_bootstrap=None):\n",
    "    \"\"\"Monkeypatch scikit learn to use per-class balancing\n",
    "\n",
    "    Private function used to fit a single tree in parallel.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose > 1:\n",
    "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
    "\n",
    "    indices = np.empty(shape=0, dtype='int64')\n",
    "    for y_class in np.unique(y):\n",
    "        sample_indices, selected = np.where(y == y_class)\n",
    "        # SELECT min_count FROM CLASS WITH REPLACEMENT\n",
    "        sample_indices = np.random.choice(sample_indices,\n",
    "                                          size=MIN_TRAIN_CLASS_COUNT, replace=True)\n",
    "        indices = np.concatenate((indices, sample_indices))\n",
    "    # IGNORE sample_weight AND SIMPLY PASS SELECTED DATA\n",
    "    tree.fit(X[indices, :], y[indices], check_input=True)\n",
    "    GLOBAL_INDICES.append(indices)\n",
    "    return tree\n",
    "\n",
    "\n",
    "def perParticipantSummaryHTML(dfParam, yTrueCol, yPredCol, pidCol, outHTML):\n",
    "    \"\"\"Provide HTML summary of how well activity classification model works\n",
    "    at the per-participant level\n",
    "\n",
    "    :param dataframe dfParam: Input pandas dataframe\n",
    "    :param str yTrueCol: Input for y_true column label\n",
    "    :param str yPregCol: Input for y_pred column label\n",
    "    :param str pidCol: Input for participant ID column label\n",
    "    :param str outHTML: Output file to print HTML summary to\n",
    "\n",
    "    :return: HTML file reporting kappa, accuracy, and confusion matrix\n",
    "    :rtype: void\n",
    "    \"\"\"\n",
    "    # get kappa & accuracy on a per-participant basis\n",
    "    pIDs = dfParam[pidCol].unique()\n",
    "    pIDKappa = []\n",
    "    pIDAccuracy = []\n",
    "    for pID in pIDs:\n",
    "        d_tmp = dfParam[dfParam[pidCol] == pID]\n",
    "        pIDKappa += [metrics.cohen_kappa_score(d_tmp[yTrueCol], d_tmp[yPredCol])]\n",
    "        pIDAccuracy += [metrics.accuracy_score(d_tmp[yTrueCol], d_tmp[yPredCol])]\n",
    "    d_summary = pd.DataFrame()\n",
    "    d_summary['pid'] = pIDs\n",
    "    d_summary['kappa'] = pIDKappa\n",
    "    d_summary['accuracy'] = pIDAccuracy\n",
    "    # print out values to html string\n",
    "    kappaSDHTML = \"Mean Kappa (SD) = \"\n",
    "    kappaSDHTML += utils.meanSDstr(d_summary['kappa'].mean(),\n",
    "                                      d_summary['kappa'].std(), 2)\n",
    "    accuracySDHTML = \"Mean accuracy (SD) = \"\n",
    "    accuracySDHTML += utils.meanSDstr(d_summary['accuracy'].mean() * 100,\n",
    "                                         d_summary['accuracy'].std() * 100, 1) + ' %'\n",
    "    kappaCIHTML = \"Mean Kappa (95% CI) = \"\n",
    "    kappaCIHTML += utils.meanCIstr(d_summary['kappa'].mean(),\n",
    "                                      d_summary['kappa'].std(), len(d_summary), 2)\n",
    "    accuracyCIHTML = \"Mean accuracy (95% CI) = \"\n",
    "    accuracyCIHTML += utils.meanCIstr(d_summary['accuracy'].mean() * 100,\n",
    "                                         d_summary['accuracy'].std() * 100, len(d_summary), 1) + ' %'\n",
    "\n",
    "    # get confusion matrix to pandas dataframe\n",
    "    y_true = dfParam[yTrueCol]\n",
    "    y_pred = dfParam[yPredCol]\n",
    "    labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "    \n",
    "    #!!CHANGED W/ JAMES\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    df_confusion = pd.DataFrame(data=cnf_matrix, columns=labels, index=labels)\n",
    "    confusionHTML = df_confusion.to_html()\n",
    "\n",
    "    # construct final output string\n",
    "    htmlStr = '<html><head><title>Classification summary</title></head><body>'\n",
    "    htmlStr += kappaSDHTML + '<br>\\n' + accuracySDHTML + '<br><br>\\n'\n",
    "    htmlStr += kappaCIHTML + '<br>\\n' + accuracyCIHTML + '<br>\\n'\n",
    "    htmlStr += confusionHTML + '<br>\\n'\n",
    "    htmlStr += '</body></html>'\n",
    "\n",
    "    # write HTML file\n",
    "    w = open(outHTML, 'w')\n",
    "    w.write(htmlStr)\n",
    "    w.close()\n",
    "\n",
    "\n",
    "def saveModelsToTar(tarArchive, featureCols, rfModel, priors, transitions,\n",
    "                    emissions, METs, featuresTxt=\"featureCols.txt\", rfModelFile=\"rfModel.pkl\",\n",
    "                    hmmPriors=\"hmmPriors.npy\", hmmEmissions=\"hmmEmissions.npy\",\n",
    "                    hmmTransitions=\"hmmTransitions.npy\", hmmMETs=\"METs.npy\"):\n",
    "    \"\"\"Save random forest and hidden markov models to tarArchive file\n",
    "\n",
    "    Note we must use the same version of python and scikit learn as in the\n",
    "    intended deployment environment\n",
    "\n",
    "    :param str tarArchive: Output tarfile\n",
    "    :param list featureCols: Input list of feature columns\n",
    "    :param sklearn.RandomForestClassifier rfModel: Input random forest model\n",
    "    :param numpy.array priors: Input prior probabilities for each activity state\n",
    "    :param numpy.array transitions: Input probability matrix of transitioning\n",
    "        from one activity state to another\n",
    "    :param numpy.array emissions: Input probability matrix of RF prediction\n",
    "        being true\n",
    "    :param numpy.array METs: Input array of average METs per activity state\n",
    "    :param str featuresTxt: Intermediate output txt file of features\n",
    "    :param str rfModelFile: Intermediate output random forest pickle model\n",
    "    :param str hmmPriors: Intermediate output HMM priors npy\n",
    "    :param str hmmEmissions: Intermediate output HMM emissions npy\n",
    "    :param str hmmTransitions: Intermediate output HMM transitions npy\n",
    "    :param str hmmMETs: Intermediate output HMM METs npy\n",
    "\n",
    "    :return: tar file of RF + HMM written to tarArchive\n",
    "    :rtype: void\n",
    "    \"\"\"\n",
    "\n",
    "    wristListToTxtFile(featureCols, featuresTxt)\n",
    "    np.save(hmmPriors, priors)\n",
    "    np.save(hmmEmissions, emissions)\n",
    "    np.save(hmmTransitions, transitions)\n",
    "    np.save(hmmMETs, METs)\n",
    "    joblib.dump(rfModel, rfModelFile, compress=9)\n",
    "\n",
    "    # Create single .tar file...\n",
    "    tarOut = tarfile.open(tarArchive, mode='w')\n",
    "    tarOut.add(featuresTxt)\n",
    "    tarOut.add(hmmPriors)\n",
    "    tarOut.add(hmmEmissions)\n",
    "    tarOut.add(hmmTransitions)\n",
    "    tarOut.add(hmmMETs)\n",
    "    tarOut.add(rfModelFile)\n",
    "    tarOut.close()\n",
    "\n",
    "    # Remove intermediate files\n",
    "    os.remove(featuresTxt)\n",
    "    os.remove(hmmPriors)\n",
    "    os.remove(hmmEmissions)\n",
    "    os.remove(hmmTransitions)\n",
    "    os.remove(hmmMETs)\n",
    "    os.remove(rfModelFile)\n",
    "    print('Models saved to', tarArchive)\n",
    "\n",
    "\n",
    "def getFileFromTar(tarArchive, targetFile):\n",
    "    \"\"\"Read file from tar\n",
    "\n",
    "    This is currently more tricky than it should be see\n",
    "    https://github.com/numpy/numpy/issues/7989\n",
    "\n",
    "    :param str tarArchive: Input tarfile object\n",
    "    :param str targetFile: Target individual file within .tar\n",
    "\n",
    "    :return: file object byte stream\n",
    "    :rtype: object\n",
    "    \"\"\"\n",
    "\n",
    "    t = tarfile.open(tarArchive, 'r')\n",
    "    array_file = BytesIO()\n",
    "    array_file.write(t.extractfile(targetFile).read())\n",
    "    array_file.seek(0)\n",
    "    return array_file\n",
    "\n",
    "\n",
    "def addReferenceLabelsToNewFeatures(\n",
    "        featuresFile,\n",
    "        referenceLabelsFile,\n",
    "        outputFile,\n",
    "        featuresTxt=\"activityModels/features.txt\",\n",
    "        labelCol=\"label\", participantCol=\"participant\",\n",
    "        atomicLabelCol=\"annotation\", metCol=\"MET\"):\n",
    "    \"\"\"Append reference annotations to newly extracted feature data\n",
    "\n",
    "    This method helps add existing curated labels (from referenceLabelsFile)\n",
    "    to a file with newly extracted features (both pre-sorted by participant\n",
    "    and time).\n",
    "\n",
    "    :param str featuresFile: Input csv file of new features data, pre-sorted by time\n",
    "    :param str referenceLabelsFile: Input csv file of reference labelled data,\n",
    "        pre-sorted by time\n",
    "    :param str outputFile: Output csv file of new features data with refernce labels\n",
    "    :param str featuresTxt: Input txt file listing feature column names\n",
    "    :param str labelCol: Input label column\n",
    "    :param str participantCol: Input participant column\n",
    "    :param str atomicLabelCol: Input 'atomic' annotation e.g. 'walking with dog'\n",
    "        vs. 'walking'\n",
    "    :param str metCol: Input MET column\n",
    "\n",
    "    :return: New csv file written to <outputFile>\n",
    "    :rtype: void\n",
    "\n",
    "    :Example:\n",
    "    >>> from accelerometer import accClassification\n",
    "    >>> accClassification.addReferenceLabelsToNewFeatures(\"newFeats.csv\",\n",
    "            \"refLabels.csv\", \"newFeatsPlusLabels.csv\")\n",
    "    <file written to newFeatsPlusLabels.csv>\n",
    "    \"\"\"\n",
    "\n",
    "    # load new features file\n",
    "    featureCols = getListFromTxtFile(featuresTxt)\n",
    "    dFeat = pd.read_csv(featuresFile, usecols=featureCols + [participantCol, 'time'])\n",
    "\n",
    "    # load in reference annotations file\n",
    "    refCols = [participantCol, 'age', 'sex', 'time', atomicLabelCol, labelCol,\n",
    "               'code', metCol, 'MET_label']\n",
    "    dRef = pd.read_csv(referenceLabelsFile, usecols=refCols)\n",
    "\n",
    "    # join dataframes\n",
    "    indexCols = [participantCol, 'time']\n",
    "    dOut = dRef.set_index(indexCols).join(dFeat.set_index(indexCols), how='left')\n",
    "\n",
    "    # write out new labelled features file\n",
    "    dOut.to_csv(outputFile, index=True)\n",
    "    print('New file written to: ', outputFile)\n",
    "\n",
    "\n",
    "def wristListToTxtFile(inputList, outputFile):\n",
    "    \"\"\"Write list of items to txt file\n",
    "\n",
    "    :param list inputList: input list\n",
    "    :param str outputFile: Output txt file\n",
    "\n",
    "    :return: list of feature columns\n",
    "    :rtype: void\n",
    "    \"\"\"\n",
    "\n",
    "    f = open(outputFile, 'w')\n",
    "    for item in inputList:\n",
    "        f.write(item + '\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def getListFromTxtFile(inputFile):\n",
    "    \"\"\"Read list of items from txt file and return as list\n",
    "\n",
    "    :param str inputFile: Input file listing items\n",
    "\n",
    "    :return: list of items\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "\n",
    "    items = []\n",
    "    f = open(inputFile, 'r')\n",
    "    for l in f:\n",
    "        items.append(l.strip())\n",
    "    f.close()\n",
    "    return items\n",
    "\n",
    "\n",
    "def resolveModelPath(pathOrModelName):\n",
    "\n",
    "    if pathlib.Path(pathOrModelName).exists():\n",
    "        return pathOrModelName\n",
    "\n",
    "    else:\n",
    "        model = MODELS.get(pathOrModelName, None)\n",
    "        if model is None:\n",
    "            raise FileNotFoundError(f\"Model file {pathOrModelName} not found\")\n",
    "        if model[\"pth\"].exists():\n",
    "            return model[\"pth\"]\n",
    "        else:\n",
    "            return downloadModel(model)\n",
    "\n",
    "\n",
    "def downloadModel(model):\n",
    "    url = model[\"url\"]\n",
    "    pth = model[\"pth\"]\n",
    "\n",
    "    print(f\"Downloading {url}...\")\n",
    "\n",
    "    with urllib.request.urlopen(url) as f_src, open(pth, \"wb\") as f_dst:\n",
    "        shutil.copyfileobj(f_src, f_dst)\n",
    "\n",
    "    return pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuff/biobankAccelerometerAnalysis/test_baa/baa/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on participant(s):,  4,5\n",
      "Output predictions written to:  ./testpredictions2.csv\n"
     ]
    }
   ],
   "source": [
    "trainClassificationModel( \\\n",
    "    \"./accelerometer/activityModels/labelled-acc-epochs.csv\", \\\n",
    "    featuresTxt=\"./accelerometer/activityModels/features_clipped.txt\", \\\n",
    "    testParticipants=\"4,5\", \\\n",
    "    outputPredict=\"./testpredictions2.csv\", \\\n",
    "    rfTrees=1000, rfThreads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "d = pd.read_csv(\"./testpredictions2.csv\")\n",
    "\n",
    "# print summary to HTML file\n",
    "htmlFile = \"classificationReport.html\"\n",
    "yTrueCol = 'label'\n",
    "yPredCol = 'predicted'\n",
    "participantCol = 'participant'\n",
    "perParticipantSummaryHTML(d, yTrueCol, yPredCol,\n",
    "    participantCol, htmlFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (baa)",
   "language": "python",
   "name": "baa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
